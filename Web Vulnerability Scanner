import requests, re, urllib.parse as urlparse
from bs4 import BeautifulSoup
import validators

class Scanner:
    def __init__(self, url, ignore_links=None):
          self.sessions=requests.session()
          self.target_url=url
          self.target_links=[]
          self.links_to_ignore=ignore_links
     
   
    def request(self, url):
        try:
            return requests.get('https://' + url)
        except request.exceptions.ConnectionError:
            pass
           
    def extract_forms(self, url):
           response = self.sessions.get(url)
           parsed_html = BeautifulSoup(response.content, features='lxml')
           return parsed_html.findAll('form')

    def submit_forms(self, form, value, url):
        action = form.get("action")
        post_url=urlparse.urljoin(url, action)
        method = form.get("method")
       
        inputs_list=form.findAll("input")
        post_data = {}
        for input in inputs_list:
            input_name=input.get("name")
            input_type=input.get("type")
            input_value=input.get("value")
            if input_value == 'text':
                input_value = 'test'
            post_data[input_name] = input_value
        if method == "post":    
            return requests.post(post_url, data=post_data)
        return self.sessions.get(post_url, params=post_data)
       
    def extract_links_from(self, url):
        response=self.sessions.get(url)
        parsed_html = BeautifulSoup(response.content, features="lxml")
        target_list = parsed_html.findAll('a')
        for target in target_list:
            valid=validators.url(self.clean(target.decode()))
            if valid and len(self.target_links)<30 and target not in self.target_links:
                self.target_links.append(self.clean(str(target)))
                print("Link = " + self.clean(target.decode()))
        return list(set(self.target_links))

   
    def clean(self, string):
        link = re.findall(r'"([^"]*)"', string)[0]
        return str(link)

   
    def crawler(self, url):
        if url==None:
            url=self.target_url
        href_links=self.extract_links_from(url)
        for link in href_links:
            link=urlparse.urljoin(url, link)
            if '=' in link:
                link=link.split("#")[0]
             
            if  self.target_url in link and link not in self.target_links:
                self.target_links.append(link)
                self.crawler(link)

    def run_scanner(self):
        print("Scanning")
        self.crawler(self.target_url)
        for link in self.target_links:
            forms = self.extract_forms(link)
            for form in forms:
                print('[+] Testing forms in ' + link)
                is_vulnerable_to_xss = self.test_xss_in_form(form, link)
                if is_vulnerable_to_xss:
                    print('[***] XSS discovered in ' + link + ' in the following form')
                    print(form)
                if "=" in link:
                    print('[+] Testing ' + link)
                    is_vulnerable_to_xss = self.test_xss_in_link(link)
                    if is_vulnerable_to_xss:
                         print('[***] XSS discovered in ' + link)
     
    def test_xss_in_form(self, form, url):
        xss_test_script = "<sCript>alert('test')</scriPt>"
        response = self.submit_forms(form, xss_test_script, url)
        if xss_test_script in response.content.decode():
            return True
             
    def test_xss_in_link(self, url):
          xss_test_script = "<sCript>alert('test')</scriPt>"
          url = url.replace("=", "=" + xss_test_script)
          response = self.sessions.get(url)
          if xss_test_script in response.content:
              return True
       
url = input('Enter url --> ')
webScanner = Scanner(url)
webScanner.run_scanner()
