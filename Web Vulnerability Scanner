import requests, re, urllib.parse as urlparse, Beautifulsoup as Beautifulsoup


target_url = target
target_links = []
data_dict=['username':USERNAME, 'password':PASSWORD,'login':BUTTON]

parsed_html = BeautifulSoup(response.content)
forms_list=parsed_html.findAll('form')
print(forms_list)

class Scanner:
      def __init__(self, url, ignore_links)
      self.sessions=requests.session()
      self.target_url=url
      self.target_links=()
      self.links_to_ignore=ignore_links
      
    
def request(url):
    try:
        return requests.get('https://' + url)
    except request.exceptions.ConnectionError:
        pass
        
def extract_forms(self, url):
       response = self.session.get(target_url)
       parsed_html = Beautifulsoup(response.content)
       return parsed_html.findAll('form')

def submit_forms(self, form, value, url):
    action = form.get("action")
    post_url=urlparse.urljoin(target_url, action)
    method = form.get("method")
    
    inputs_list=form.findAll("input")
    post_data = []
    for input in inputs_list:
        input_name=input.get("name")
        input_type=input.get("type")
        input_value=input.get("value")
        if input_value == 'text':
            input_value = 'test'
        post_data[input_name] = input_value
    if method == "post":    
        return requests.post(post_url, data=post_data)
    return self.session.get(post_url, params=post_data)
    
def extract_links_from(url):
    response=self.session.get(url)
    return re.findall('(?:href)(.*?)"', response.content.decode(errors="ignore"))

def crawler(url=None)
    if url==None:
        url=self.target_url
    href_links=self.extract_links_from(url)
    for link in href_links:
        link=urlparse.urljoin(url, link)
    
    if '=' in link:
        link=link.split("#")[0]
        
    if self.target_url in link and link not in target_links and link not in self.links_to_ignore:
        self.target_links.append(link)
        print(link)
        self.crawl(link)

def run_scanner(self):
    for link in self.target_links:
        forms = self.extract_gorms(link)
        for form in forms:
            print('[+] Testing form in " + link)
            is_vulnerable_to_xss = self.test_xss_in_form(form, link)
            if is_vulnerable_to_xss:
                print("[***] XSS discovered in ' + link + ' in the following form')
                print(form)
         if "=" in link:
            print('[+] Testing ' + link)
            is_vulnerable_to_xss = self.test_xss_in_link(link)
            if is_vulnerable_to_xss:
                 print("[***] XSS discovered in ' + link
 
 def test_xss_in_form(self, form, url):
      xss_test_script = "<sCript>alert('test')</scriPt>"
      response = self.submit_forms(form, xss_test_script, url)
      if xss_test_script in response.content:
          return True
          
def test_xss_in_link(self, url):
      xss_test_script = "<sCript>alert('test')</scriPt>"
      url = url.replace("=", "=" + xss_test_script)
      response = self.session.get(url)
       if xss_test_script in response.content:
          return True
