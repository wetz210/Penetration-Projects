import requests, re, urllib.parse as urlparse
from bs4 import BeautifulSoup
import validators

class Scanner:
    def __init__(self, url, limit, ignore_links=None):
          self.sessions=requests.session()
          self.target_url=url
          self.target_links=[]
          self.links_to_ignore=ignore_links
          self.base_url = url.split('.')[1]
          self.limit = limit
          self.vul_count = 0
     
   
    def request(self, url):
        try:
            return requests.get('https://' + url)
        except request.exceptions.ConnectionError:
            pass
           
    def extract_forms(self, url):
           response = self.sessions.get(url)
           parsed_html = BeautifulSoup(response.content, features='lxml')
           return parsed_html.findAll('form')

    def submit_forms(self, form, value, url):
        action = form.get("action")
        post_url=urlparse.urljoin(url, action)
        method = form.get("method")
       
        inputs_list=form.findAll("input")
        post_data = {}
        for input in inputs_list:
            input_name=input.get("name")
            input_type=input.get("type")
            input_value=input.get("value")
            if input_value == 'text':
                input_value = 'test'
            post_data[input_name] = input_value
        if method == "post":    
            return requests.post(post_url, data=post_data)
        return self.sessions.get(post_url, params=post_data)
       
    def extract_links_from(self, url):
        reqs = requests.get(url)
        soup = BeautifulSoup(reqs.text, 'html.parser')
        for link in soup.find_all('a'):
            clean_link = link.get('href')
            valid=validators.url(str(clean_link))
            base=str(clean_link).split('.')
            if valid and self.base_url in str(link).split('.') and clean_link not in self.target_links and 'download' not in clean_link and len(self.target_links)<int(self.limit):
                print(clean_link)
                self.target_links.append(clean_link)
                self.extract_links_from(clean_link)
        return list(set(self.target_links))

   
    def crawler(self, url):
        if url==None:
            url=self.target_url
        href_links=self.extract_links_from(url)
        for link in href_links:
            link=urlparse.urljoin(url, link)
            if '=' in link:
                link=link.split("#")[0]
                
            if  self.target_url in link and link not in self.target_links:
                self.target_links.append(link)
                self.crawler(link)

    def run_scanner(self):
        self.crawler(self.target_url)
        print('[+] Initiating tests')
        for link in self.target_links:
            forms = self.extract_forms(link)
            for form in forms:
                is_vulnerable_to_xss = self.test_xss_in_form(form, link)
                is_vulnerable_to_sqli = self.test_sqli_in_form(form, link)
                if is_vulnerable_to_sqli:
                   print('[***] SQLi discovered in ' + link)
                   self.vul_count += 1
                
                if is_vulnerable_to_xss:
                    print('[***] XSS discovered in ' + link)
                    self.vul_count += 1
                if "=" in link:
                    is_vulnerable_to_xss = self.test_xss_in_link(link)
                    if is_vulnerable_to_xss:
                         print('[***] XSS discovered in ' + link)
                         self.vul_count += 1
                         
        print('Scan ended, ' + str(self.vul_count) + ' vulnerabilities found')
     
    def test_xss_in_form(self, form, url):
        xss_test_script = "<sCript>alert('test')</scriPt>"
        response = self.submit_forms(form, xss_test_script, url)
        if xss_test_script in response.content.decode():
            return True
             
    def test_xss_in_link(self, url):
          xss_test_script = "<sCript>alert('test')</scriPt>"
          url = url.replace("=", "=" + xss_test_script)
          response = self.sessions.get(url)
          if xss_test_script in response.content:
              return True
              
                   
    def test_sqli_in_form(self, form, url):
        sqli_test_script = "' or '1'='1"
        response = self.submit_forms(form, sqli_test_script, url)
        if 'table' in response.content.decode():
            return True
       
url = input('Enter url: ')
limit = input('Enter number of links to scan: ')
webScanner = Scanner(url, limit)
webScanner.run_scanner()
